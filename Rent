import pandas as pd
import numpy as np

## --- 1. ENHANCED DATASETS (20 NCR MICRO-MARKETS) ---

def get_enhanced_office_data():
    """
    Simulates enhanced data extraction for 20 micro-markets across Delhi-NCR,
    using varied proxy benchmarks for a robust hackathon model.
    """
    # Micro-Market Data (Delhi, Gurugram, Noida)
    data = {
        'Micro_Market': [
            'Delhi CBD (Connaught Place)', 'South-East Delhi (Nehru Place)', 'Delhi Aerocity (DIAL)', 
            'Okhla Industrial Area', 'Janakpuri District Centre', 'Netaji Subhash Place',
            'Cyber City (Gurugram)', 'Golf Course Road (Gurugram)', 'Sohna Road (Gurugram)', 
            'NH-8 / Delhi-Jaipur Highway (Gurugram)', 'Sector 32/44 (Gurugram)', 'Udyog Vihar (Gurugram)',
            'Noida Expressway (Sect 125/135)', 'Sector 62 (Noida)', 'Sector 16 (Noida)', 
            'Greater Noida West (Noida)', 'Sector 18 (Noida)', 'Noida - Greater Noida Link Road',
            'Central Noida (Sector 58/63)', 'Laxmi Nagar (East Delhi)'
        ],
        
        # 1. Real Estate Proxies (Quantitative) - Varied based on Market Tier
        'Avg_Rent_Per_SqFt': [284.45, 115.60, 223.91, 65.20, 95.00, 105.00, 
                              135.00, 110.50, 80.00, 75.00, 90.00, 85.00, 
                              75.00, 85.50, 99.00, 55.00, 120.00, 60.00, 
                              70.00, 82.00], 
        
        # 2. Market Dynamics (Key Indicators)
        'Vacancy_Rate_Percent': [18.3, 13.0, 26.8, 28.0, 15.0, 12.0, 
                                 2.2, 11.5, 18.0, 15.5, 10.0, 8.5, 
                                 12.0, 16.0, 10.5, 35.0, 14.0, 25.0, 
                                 22.0, 9.0],       
        'Net_Absorption_YTD_MSF': [0.05, 0.08, 0.51, 0.02, 0.15, 0.10, 
                                   0.10, 0.35, 0.25, 0.40, 0.30, 0.05, 
                                   0.85, 0.45, 0.10, 0.01, 0.05, 0.03, 
                                   0.60, 0.15],     
        'Grade_A_Stock_MSF': [1.53, 7.09, 2.42, 0.80, 0.50, 1.20, 
                              13.99, 5.50, 3.20, 6.00, 2.80, 4.00, 
                              10.50, 4.50, 2.00, 1.50, 0.90, 2.50, 
                              3.80, 0.70],        

        # 3. Popularity/Qualitative Proxies (Synthetic/Manual)
        'Metro_Connectivity_Score': [5, 4, 5, 2, 3, 4, 
                                     4, 3, 2, 3, 3, 4, 
                                     3, 4, 5, 1, 5, 2, 
                                     3, 5],                  
        'Previous_Occupier_Rating': [4.5, 3.8, 4.2, 3.0, 3.5, 4.0, 
                                     4.7, 4.5, 3.2, 4.0, 4.3, 3.9, 
                                     3.5, 4.1, 4.0, 2.8, 4.4, 3.1, 
                                     3.6, 3.7]         
    }
    df = pd.DataFrame(data)
    return df

## --- 2. BUSINESS SUITABILITY ANALYSIS (ADVANCED WEIGHTING) ---
# The scoring logic remains the same for consistency, ensuring a fair comparison.
def calculate_suitability_score(df):
    
    # Weights for the Hackathon Model (Sum of absolute values is 1.0)
    WEIGHTS = {
        'Rent': -0.30,           
        'Vacancy': -0.15,        
        'Absorption': 0.20,      
        'Maturity': 0.05,        
        'Connectivity': 0.15,    
        'Rating': 0.15           
    }

    # Normalization Function (Min-Max Scaler 0-1)
    def min_max_normalize(series, inverse=False):
        min_val = series.min()
        max_val = series.max()
        range_val = max_val - min_val
        
        if range_val == 0:
            return pd.Series([0.5] * len(series), index=series.index) 

        if inverse:
            # For metrics like Rent/Vacancy (lower is better)
            return (max_val - series) / range_val
        else:
            # For metrics like Absorption/Scores (higher is better)
            return (series - min_val) / range_val

    # Apply Normalization and Weighting
    
    # Negative Factors (Normalized inversely)
    df['N_Rent'] = min_max_normalize(df['Avg_Rent_Per_SqFt'], inverse=True) * abs(WEIGHTS['Rent'])
    df['N_Vacancy'] = min_max_normalize(df['Vacancy_Rate_Percent'], inverse=True) * abs(WEIGHTS['Vacancy'])

    # Positive Factors (Normalized directly)
    df['N_Absorption'] = min_max_normalize(df['Net_Absorption_YTD_MSF']) * WEIGHTS['Absorption']
    df['N_Maturity'] = min_max_normalize(df['Grade_A_Stock_MSF']) * WEIGHTS['Maturity']
    df['N_Connectivity'] = min_max_normalize(df['Metro_Connectivity_Score']) * WEIGHTS['Connectivity']
    df['N_Rating'] = min_max_normalize(df['Previous_Occupier_Rating']) * WEIGHTS['Rating']
    
    # Calculate Final Suitability Score (Sum of all normalized, weighted scores)
    score_columns = [col for col in df.columns if col.startswith('N_')]
    df['Suitability_Score'] = df[score_columns].sum(axis=1)

    # Sort and select relevant columns for output
    df = df.sort_values(by='Suitability_Score', ascending=False)
    
    return df

# --- EXECUTION ---
if __name__ == '__main__':
    # 1. Extract/Simulate Enhanced Data
    office_df = get_enhanced_office_data()
    print("## üè¢ 20 NCR Micro-Market Suitability Analysis")
    print(f"Total Markets Analyzed: {len(office_df)}")
    print("---")
    
    # 2. Analyze and Score
    results_df = calculate_suitability_score(office_df.copy())
    
    # 3. Present Results (Top 10 Locations)
    print("## ‚úÖ Top 10 Business Suitability Recommendations")
    
    final_cols = ['Micro_Market', 'Avg_Rent_Per_SqFt', 'Vacancy_Rate_Percent', 'Net_Absorption_YTD_MSF', 'Suitability_Score']
    
    # Print the resulting table for the top 10
    print(results_df.head(10)[final_cols].round(4).to_markdown(index=False))
    print("---")
    
    # Highlight the Top Recommendation
    top_recommendation = results_df.iloc[0]
    print(f"\nü•á **Top Recommended Location:** {top_recommendation['Micro_Market']}")
    print(f"   (Suitability Score: **{top_recommendation['Suitability_Score']:.4f}**)")
